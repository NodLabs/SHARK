model_name, use_tracing, model_type
valhalla/bart-large-sst2,True,hf
l-yohai/bigbird-roberta-base-mnli,True,hf
hf-internal-testing/tiny-random-bigbird_pegasus,True,hf
bigscience/bloom-350m,True,hf
google/canine-s,True,hf
YituTech/conv-bert-base,True,hf
ctrl,True,hf
facebook/data2vec-text-base,True,hf
microsoft/deberta-base,True,hf
distilbert-base-uncased,True,hf
microsoft/deberta-v2-xlarge,True,hf
distilbert-base-uncased,True,hf
bhadresh-savani/electra-base-emotion,True,hf
xlm-mlm-en-2048,True,hf
google/fnet-base,True,hf



Here is the debug info, each model name follow with a Traceback  when run python generate_sharktank.py
models is from list of Huggingface AutoModelForSequenceClassification
https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification:~:text=config%0A...%20)-,AutoModelForSequenceClassification,-class%20transformers.


######################################################################################################################################
valhalla/bart-large-sst2,True,hf

➜  SHARK git:(add-torch-tests) ✗ python generate_sharktank.py
2022-08-04 09:59:55.845686: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-08-04 09:59:55.845702: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-08-04 09:59:56.927814: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW
2022-08-04 09:59:56.927841: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: nod
2022-08-04 09:59:56.927846: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: nod
2022-08-04 09:59:56.927886: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.141.3
2022-08-04 09:59:56.927904: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.129.6
2022-08-04 09:59:56.927909: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 470.129.6 does not match DSO version 470.141.3 -- cannot find working devices in this configuration
Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 63, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    actual_out = model(test_input)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 46, in forward
    return self.model.forward(tokens)[0]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1513, in forward
    sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[
IndexError: index -1 is out of bounds for dimension 1 with size 0








######################################################################################################################################
l-yohai/bigbird-roberta-base-mnli,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 63, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 55, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 37, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 446, in from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1882, in from_pretrained
    model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2045, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for BigBirdForSequenceClassification:
	size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).
	size mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2]).








######################################################################################################################################
hf-internal-testing/tiny-random-bigbird_pegasus,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 76, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: 'func.call' op operand type mismatch: expected operand type '!torch.float', but provided '!torch.number' for operand number 1
note: see current operation: %499 = "func.call"(%493, %249, %494, %495, %496, %497, %498) {callee = @__torch_mlir_shape_fn.aten.arange.start_step} : (!torch.float, !torch.number, !torch.float, !torch.optional<int>, !torch.optional<int>, !torch.optional<Device>, !torch.optional<bool>) -> !torch.list<int>

Error can be reproduced with:
$ torch-mlir-opt -pass-pipeline='torchscript-module-to-torch-backend-pipeline' /tmp/HuggingFaceLanguage.mlir
Add '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.








######################################################################################################################################
bigscience/bloom-350m,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 63, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 55, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 37, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 423, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 672, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 387, in __getitem__
    raise KeyError(key)
KeyError: 'bloom'









######################################################################################################################################
google/canine-s,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 213, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception: 
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: operand types should have the same type as the list contained type
note: see current operation: %495 = "torch.prim.ListConstruct"(%493, %494) : (!torch.vtensor<[1,1,768],f32>, !torch.vtensor<[1,31,768],f32>) -> !torch.list<vtensor<[1,1,768],f32>>









######################################################################################################################################
YituTech/conv-bert-base,True,hf
Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 76, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: failed to legalize operation 'torch.aten.reshape' that was explicitly marked illegal
note: see current operation: %393 = "torch.aten.reshape"(%391, %392) : (!torch.tensor<[1,128,384],f32>, !torch.list<int>) -> !torch.tensor<[1,128,384],f32>

Error can be reproduced with:
$ torch-mlir-opt -pass-pipeline='torchscript-module-to-torch-backend-pipeline' /tmp/HuggingFaceLanguage.mlir
Add '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.








######################################################################################################################################
ctrl,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 76, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: 'func.call' op operand type mismatch: expected operand type '!torch.float', but provided '!torch.number' for operand number 1
note: see current operation: %3055 = "func.call"(%3049, %801, %3050, %3051, %3052, %3053, %3054) {callee = @__torch_mlir_shape_fn.aten.arange.start_step} : (!torch.float, !torch.number, !torch.float, !torch.optional<int>, !torch.optional<int>, !torch.optional<Device>, !torch.optional<bool>) -> !torch.list<int>

Error can be reproduced with:
$ torch-mlir-opt -pass-pipeline='torchscript-module-to-torch-backend-pipeline' /tmp/HuggingFaceLanguage.mlir
Add '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.








######################################################################################################################################
facebook/data2vec-text-base,True,hf

Some weights of Data2VecTextForSequenceClassification were not initialized from the model checkpoint at facebook/data2vec-text-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Fatal Python error: Segmentation fault
Current thread 0x00007f8b8a2de740 (most recent call first):
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 30 in run_pipeline_with_repro_report
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 164 in compile
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123 in get_torch_mlir_module
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74 in _torch_mlir
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109 in import_mlir
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163 in import_debug
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 76 in save_torch_model
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215 in <module>







######################################################################################################################################
microsoft/deberta-base,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 76, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: 'torch.derefine' op operand type '!torch.number' and result type '!torch.optional<float>' are cast incompatible
note: see current operation: %1828 = "torch.derefine"(%329) : (!torch.number) -> !torch.optional<float>

Error can be reproduced with:
$ torch-mlir-opt -pass-pipeline='torchscript-module-to-torch-backend-pipeline' /tmp/HuggingFaceLanguage.mlir
Add '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.







######################################################################################################################################
microsoft/deberta-v2-xlarge,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 76, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: failed to legalize operation 'torch.aten.to.dtype_layout' that was explicitly marked illegal
note: see current operation: %489 = "torch.aten.to.dtype_layout"(%488, %411, %403, %427, %413, %412, %412, %413) : (!torch.tensor<[1,1,128,128],si64>, !torch.int, !torch.int, !torch.Device, !torch.none, !torch.bool, !torch.bool, !torch.none) -> !torch.tensor<[1,1,128,128],si64>






######################################################################################################################################
distilbert-base-uncased,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 89, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 150, in get_torch_mlir_module
    pm.run(mb.module)
RuntimeError: Failure while executing pass pipeline.






######################################################################################################################################
bhadresh-savani/electra-base-emotion,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 63, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 55, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 37, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 446, in from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1882, in from_pretrained
    model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2045, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for ElectraForSequenceClassification:
	size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([6, 256]) from checkpoint, the shape in current model is torch.Size([2, 256]).
	size mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([6]) from checkpoint, the shape in current model is torch.Size([2]).






######################################################################################################################################
xlm-mlm-en-2048,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 215, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 76, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: 'func.call' op operand type mismatch: expected operand type '!torch.float', but provided '!torch.number' for operand number 0
note: see current operation: %793 = "func.call"(%221, %789, %790, %791, %792) {callee = @__torch_mlir_shape_fn.aten.arange} : (!torch.number, !torch.optional<int>, !torch.optional<int>, !torch.optional<Device>, !torch.optional<bool>) -> !torch.list<int>




######################################################################################################################################
fnet — FNetForSequenceClassification (FNet model)
google/fnet-base,True,hf

error: unsupported PyTorch scalar type: ComplexFloat
Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 144, in compile
    mb.import_module(scripted._c, class_annotator)
RuntimeError: unsupported type








######################################################################################################################################
funnel — FunnelForSequenceClassification (Funnel Transformer model)
funnel-transformer/small-base,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: operand types should have the same type as the list contained type
note: see current operation: %208 = "torch.prim.ListConstruct"(%206, %207) : (!torch.vtensor<[1],f32>, !torch.vtensor<[63],f32>) -> !torch.list<vtensor<[1],f32>>











######################################################################################################################################
gpt2 — GPT2ForSequenceClassification (OpenAI GPT-2 model)
microsoft/DialogRPT-updown,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 446, in from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1882, in from_pretrained
    model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2045, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for GPT2ForSequenceClassification:
	size mismatch for score.weight: copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape in current model is torch.Size([2, 1024]).





######################################################################################################################################
gpt_neo — GPTNeoForSequenceClassification (GPT Neo model)
EleutherAI/gpt-neo-1.3B,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 144, in compile
    mb.import_module(scripted._c, class_annotator)
ValueError: Unsupported import tensor type: (1,1,.,.) =
 Columns 1 to 26  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  [ CPUByteType{1,1,2048,2048} ]







######################################################################################################################################
gptj — GPTJForSequenceClassification (GPT-J model)
ydshieh/tiny-random-gptj-for-sequence-classification,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 144, in compile
    mb.import_module(scripted._c, class_annotator)
ValueError: Unsupported import tensor type: (1,1,.,.) =....
[ CPUByteType{1,1,512,512} ]




######################################################################################################################################
ibert — IBertForSequenceClassification (I-BERT model)
kssteven/ibert-roberta-base,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: failed to legalize operation 'torch.aten.to.dtype_layout' that was explicitly marked illegal
note: see current operation: %245 = "torch.aten.to.dtype_layout"(%244, %213, %206, %228, %222, %210, %210, %222) : (!torch.vtensor<[1,128],si64>, !torch.int, !torch.int, !torch.Device, !torch.none, !torch.bool, !torch.bool, !torch.none) -> !torch.vtensor<[1,128],si64>






######################################################################################################################################
layoutlmv2 — LayoutLMv2ForSequenceClassification (LayoutLMv2 model)
microsoft/layoutlmv2-base-uncased,True,hf
python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 61, in get_hf_model
    actual_out = model(test_input)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 49, in forward
    return self.model.forward(tokens)[0]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py", line 1032, in forward
    visual_bbox = self.layoutlmv2._calc_visual_bbox(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py", line 778, in _calc_visual_bbox
    dtype=bbox.dtype,
AttributeError: 'NoneType' object has no attribute 'dtype'



######################################################################################################################################
layoutlmv3 — LayoutLMv3ForSequenceClassification (LayoutLMv3 model)
microsoft/layoutlmv3-base,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 423, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 672, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 387, in __getitem__
    raise KeyError(key)
KeyError: 'layoutlmv3'



######################################################################################################################################
led — LEDForSequenceClassification (LED model)
allenai/led-base-16384,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 61, in get_hf_model
    actual_out = model(test_input)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 49, in forward
    return self.model.forward(tokens)[0]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/led/modeling_led.py", line 2544, in forward
    sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[
IndexError: index -1 is out of bounds for dimension 1 with size 0
➜  SHARK git:(add-torch-tests) ✗



######################################################################################################################################
longformer — LongformerForSequenceClassification (Longformer model)
jpwahle/longformer-base-plagiarism-detection,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: failed to legalize operation 'torch.aten.index_put_' that was explicitly marked illegal
note: see current operation: %934 = "torch.aten.index_put_"(%929, %930, %933, %34) : (!torch.tensor<[1,512,12,1],f32>, !torch.list<optional<tensor>>, !torch.tensor<[],f32>, !torch.bool) -> !torch.tensor<[1,512,12,1],f32>




######################################################################################################################################
luke — LukeForSequenceClassification (LUKE model)
studio-ousia/luke-base,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 447, in from_pretrained
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.luke.configuration_luke.LukeConfig'> for this kind of AutoModel: AutoModelForSequenceClassification.
Model type should be one of YosoConfig, NystromformerConfig, QDQBertConfig, FNetConfig, PerceiverConfig, GPTJConfig, LayoutLMv2Config, PLBartConfig, RemBertConfig, CanineConfig, RoFormerConfig, BigBirdPegasusConfig, GPTNeoConfig, BigBirdConfig, ConvBertConfig, LEDConfig, IBertConfig, MobileBertConfig, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaXLConfig, XLMRobertaConfig, MBartConfig, MegatronBertConfig, MPNetConfig, BartConfig, ReformerConfig, LongformerConfig, RobertaConfig, DebertaV2Config, DebertaConfig, FlaubertConfig, SqueezeBertConfig, BertConfig, OpenAIGPTConfig, GPT2Config, TransfoXLConfig, XLNetConfig, XLMConfig, CTRLConfig, ElectraConfig, FunnelConfig, LayoutLMConfig, TapasConfig, Data2VecTextConfig.



######################################################################################################################################
mbart — MBartForSequenceClassification (mBART model)
hf-internal-testing/tiny-random-mbart,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 61, in get_hf_model
    actual_out = model(test_input)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 49, in forward
    return self.model.forward(tokens)[0]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py", line 1482, in forward
    outputs = self.model(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py", line 1203, in forward
    encoder_outputs = self.encoder(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py", line 792, in forward
    embed_pos = self.embed_positions(input_shape)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py", line 142, in forward
    return super().forward(positions + self.offset)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 158, in forward
    return F.embedding(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 2155, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self



######################################################################################################################################
megatron-bert — MegatronBertForSequenceClassification (Megatron-BERT model)
nvidia/megatron-bert-cased-345m,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 423, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 621, in _get_config_dict
    raise EnvironmentError(
OSError: nvidia/megatron-bert-cased-345m does not appear to have a file named config.json.



######################################################################################################################################
mobilebert — MobileBertForSequenceClassification (MobileBERT model)
lordtt13/emo-mobilebert,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 446, in from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1882, in from_pretrained
    model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2045, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for MobileBertForSequenceClassification:
	size mismatch for classifier.weight: copying a param with shape torch.Size([4, 512]) from checkpoint, the shape in current model is torch.Size([2, 512]).
	size mismatch for classifier.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([2]).



######################################################################################################################################
mpnet — MPNetForSequenceClassification (MPNet model)
microsoft/mpnet-base,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: 'func.call' op operand type mismatch: expected operand type '!torch.float', but provided '!torch.number' for operand number 0
note: see current operation: %893 = "func.call"(%261, %889, %890, %891, %892) {callee = @__torch_mlir_shape_fn.aten.arange} : (!torch.number, !torch.optional<int>, !torch.optional<int>, !torch.optional<Device>, !torch.optional<bool>) -> !torch.list<int>




######################################################################################################################################
mvp — MvpForSequenceClassification (MVP model)
RUCAIBox/mvp,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 423, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 672, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 387, in __getitem__
    raise KeyError(key)
KeyError: 'mvp'



######################################################################################################################################
nezha — NezhaForSequenceClassification (Nezha model)
sijunhe/nezha-cn-base,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 423, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 672, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 387, in __getitem__
    raise KeyError(key)
KeyError: 'nezha'





######################################################################################################################################
nystromformer — NystromformerForSequenceClassification (Nyströmformer model)
uw-madison/nystromformer-512,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 164, in compile
    run_pipeline_with_repro_report(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR failed with the following diagnostics:
error: unsupported by backend lowering: `torch.operator` op
note: see current operation: %305 = "torch.operator"(%278, %301, %235, %302, %303, %302, %226, %304, %237, %226, %226, %238, %238) {name = "aten._convolution"} : (!torch.tensor<[1,12,128,64],f32>, !torch.tensor<[12,1,65,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int, !torch.bool, !torch.bool, !torch.bool, !torch.bool) -> !torch.tensor<[1,12,128,64],f32>
note: this is likely due to a missing op that needs to be generated by torch_ods_gen.py




######################################################################################################################################
openai-gpt — OpenAIGPTForSequenceClassification (OpenAI GPT model)
openai-gpt,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 164, in compile
    run_pipeline_with_repro_report(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR failed with the following diagnostics:
error: unsupported by backend lowering: `torch.operator` op
note: see current operation: %199 = "torch.operator"(%198, %161, %169) {name = "aten.split.Tensor"} : (!torch.tensor<[1,128,2304],f32>, !torch.int, !torch.int) -> !torch.list<tensor>
note: this is likely due to a missing op that needs to be generated by torch_ods_gen.py




######################################################################################################################################
opt — OPTForSequenceClassification (OPT model)
facebook/opt-350m,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 423, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 672, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 387, in __getitem__
    raise KeyError(key)
KeyError: 'opt'



######################################################################################################################################
perceiver — PerceiverForSequenceClassification (Perceiver model)
deepmind/language-perceiver,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: 'func.call' op operand type mismatch: expected operand type '!torch.float', but provided '!torch.number' for operand number 1
note: see current operation: %1833 = "func.call"(%1827, %479, %1828, %1829, %1830, %1831, %1832) {callee = @__torch_mlir_shape_fn.aten.arange.start_step} : (!torch.float, !torch.number, !torch.float, !torch.optional<int>, !torch.optional<int>, !torch.optional<Device>, !torch.optional<bool>) -> !torch.list<int>



######################################################################################################################################
plbart — PLBartForSequenceClassification (PLBart model)
hf-internal-testing/tiny-plbart,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 61, in get_hf_model
    actual_out = model(test_input)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 49, in forward
    return self.model.forward(tokens)[0]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py", line 1474, in forward
    sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[
IndexError: index -1 is out of bounds for dimension 1 with size 0




######################################################################################################################################
qdqbert — QDQBertForSequenceClassification (QDQBert model)
bert-base-uncased,True,hf




######################################################################################################################################
reformer — ReformerForSequenceClassification (Reformer model)
hf-internal-testing/tiny-random-reformer,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 61, in get_hf_model
    actual_out = model(test_input)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 49, in forward
    return self.model.forward(tokens)[0]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 2422, in forward
    outputs = self.reformer(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 2079, in forward
    embedding_output = self.embeddings(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 265, in forward
    position_embeddings = self.position_embeddings(position_ids)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 182, in forward
    raise ValueError(
ValueError: Make sure that config.axial_pos_shape factors: (4, 25) multiply at least to max(sequence_length, least_common_mult_chunk_length): max(128, 4).



######################################################################################################################################
rembert — RemBertForSequenceClassification (RemBERT model)
hf-internal-testing/tiny-random-reformer,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 61, in get_hf_model
    actual_out = model(test_input)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 49, in forward
    return self.model.forward(tokens)[0]
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 2422, in forward
    outputs = self.reformer(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 2079, in forward
    embedding_output = self.embeddings(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 265, in forward
    position_embeddings = self.position_embeddings(position_ids)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1131, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 182, in forward
    raise ValueError(
ValueError: Make sure that config.axial_pos_shape factors: (4, 25) multiply at least to max(sequence_length, least_common_mult_chunk_length): max(128, 4).



######################################################################################################################################
roberta — RobertaForSequenceClassification (RoBERTa model)
cardiffnlp/twitter-roberta-base-emotion,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 446, in from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1882, in from_pretrained
    model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2045, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for RobertaForSequenceClassification:
	size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([4, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).
	size mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([2]).



######################################################################################################################################
roformer — RoFormerForSequenceClassification (RoFormer model)
junnyu/roformer_chinese_base,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: 'func.call' op operand type mismatch: expected operand type '!torch.float', but provided '!torch.number' for operand number 1
note: see current operation: %1415 = "func.call"(%1409, %250, %1410, %1411, %1412, %1413, %1414) {callee = @__torch_mlir_shape_fn.aten.arange.start_step} : (!torch.float, !torch.number, !torch.float, !torch.optional<int>, !torch.optional<int>, !torch.optional<Device>, !torch.optional<bool>) -> !torch.list<int>




######################################################################################################################################
squeezebert — SqueezeBertForSequenceClassification (SqueezeBERT model)
squeezebert/squeezebert-uncased,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 164, in compile
    run_pipeline_with_repro_report(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR failed with the following diagnostics:
error: unsupported by backend lowering: `torch.operator` op
note: see current operation: %252 = "torch.operator"(%247, %249, %248, %250, %251, %250, %204, %251, %208, %204, %204, %212, %212) {name = "aten._convolution"} : (!torch.tensor<[1,768,128],f32>, !torch.tensor<[768,192,1],f32>, !torch.tensor<[768],f32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int, !torch.bool, !torch.bool, !torch.bool, !torch.bool) -> !torch.tensor<[1,768,128],f32>
note: this is likely due to a missing op that needs to be generated by torch_ods_gen.py



######################################################################################################################################
tapas — TapasForSequenceClassification (TAPAS model)
google/tapas-base-finetuned-tabfact,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 446, in from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1843, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/tapas/modeling_tapas.py", line 1473, in __init__
    self.tapas = TapasModel(config)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/tapas/modeling_tapas.py", line 868, in __init__
    requires_backends(self, "scatter")
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 761, in requires_backends
    raise ImportError("".join(failed))
ImportError:
TapasModel requires the torch-scatter library but it was not found in your environment. You can install it with pip as
explained here: https://github.com/rusty1s/pytorch_scatter.



######################################################################################################################################
transfo-xl — TransfoXLForSequenceClassification (Transformer-XL model)
transfo-xl-wt103,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: operand types should have the same type as the list contained type
note: see current operation: %263 = "torch.prim.ListConstruct"(%261, %262) : (!torch.vtensor<[1600,1,1024],f32>, !torch.vtensor<[128,1,1024],f32>) -> !torch.list<vtensor<[1600,1,1024],f32>>



######################################################################################################################################
xlm — XLMForSequenceClassification (XLM model)
xlm-mlm-en-2048,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: 'func.call' op operand type mismatch: expected operand type '!torch.float', but provided '!torch.number' for operand number 0
note: see current operation: %793 = "func.call"(%221, %789, %790, %791, %792) {callee = @__torch_mlir_shape_fn.aten.arange} : (!torch.number, !torch.optional<int>, !torch.optional<int>, !torch.optional<Device>, !torch.optional<bool>) -> !torch.list<int>



######################################################################################################################################
xlm-roberta — XLMRobertaForSequenceClassification (XLM-RoBERTa model)
cardiffnlp/twitter-roberta-base-emotion,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 446, in from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1882, in from_pretrained
    model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2045, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for RobertaForSequenceClassification:
	size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([4, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).
	size mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([2]).




######################################################################################################################################
xlm-roberta-xl — XLMRobertaXLForSequenceClassification (XLM-RoBERTa-XL model)
xlm-roberta-xlarge,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 61, in save_torch_model
    model, input, _ = get_hf_model(torch_model_name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 58, in get_hf_model
    model = HuggingFaceLanguage(name)
  File "/home/chi/src/ubuntu20/shark/SHARK/tank/model_utils.py", line 40, in __init__
    self.model = AutoModelForSequenceClassification.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 423, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like xlm-roberta-xlarge is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.



######################################################################################################################################
xlnet — XLNetForSequenceClassification (XLNet model)
xlnet-base-cased,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: 'func.call' op operand type mismatch: expected operand type '!torch.float', but provided '!torch.number' for operand number 0
note: see current operation: %1124 = "func.call"(%1064, %1120, %1121, %1122, %1123) {callee = @__torch_mlir_shape_fn.aten.arange} : (!torch.number, !torch.optional<int>, !torch.optional<int>, !torch.optional<Device>, !torch.optional<bool>) -> !torch.list<int>


Error can be reproduced with:
$ torch-mlir-opt -pass-pipeline='torchscript-module-to-torch-backend-pipeline' /tmp/HuggingFaceLanguage.mlir
Add '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.




######################################################################################################################################
yoso — YosoForSequenceClassification (YOSO model)
uw-madison/yoso-4096,True,hf

Traceback (most recent call last):
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 214, in <module>
    save_torch_model(args.torch_model_csv)
  File "/home/chi/src/ubuntu20/shark/SHARK/generate_sharktank.py", line 74, in save_torch_model
    mlir_importer.import_debug(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 163, in import_debug
    imported_mlir = self.import_mlir(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 109, in import_mlir
    return self._torch_mlir(is_dynamic, tracing_required), func_name
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/shark_importer.py", line 74, in _torch_mlir
    return get_torch_mlir_module(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark/torch_mlir_utils.py", line 123, in get_torch_mlir_module
    module = torch_mlir.compile(
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/__init__.py", line 149, in compile
    run_pipeline_with_repro_report(mb.module,
  File "/home/chi/src/ubuntu20/shark/SHARK/shark.venv/lib/python3.10/site-packages/torch_mlir/compiler_utils.py", line 49, in run_pipeline_with_repro_report
    raise Exception(f"""
Exception:
Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:
error: failed to legalize operation 'torch.aten.reshape' that was explicitly marked illegal
note: see current operation: %278 = "torch.aten.reshape"(%270, %277) : (!torch.tensor<[1,12,128,64],f32>, !torch.list<int>) -> !torch.tensor<[12,128,64],f32>


Error can be reproduced with:
$ torch-mlir-opt -pass-pipeline='torchscript-module-to-torch-backend-pipeline' /tmp/HuggingFaceLanguage.mlir
Add '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.




